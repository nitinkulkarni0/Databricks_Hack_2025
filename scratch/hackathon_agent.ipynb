import os
import pandas as pd

from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_databricks import ChatDatabricks
from databricks.sdk import WorkspaceClient
from langchain_core.runnables import Runnable, RunnableSequence, RunnableLambda


# configure workspace tokens
w = WorkspaceClient()
os.environ["DATABRICKS_HOST"] = w.config.host
os.environ["DATABRICKS_TOKEN"] = w.tokens.create(comment="for model serving", lifetime_seconds=1200).token_value

llm = ChatDatabricks(endpoint="databricks-meta-llama-3-1-405b-instruct")

def format_context(df: pd.DataFrame) -> str:
    return df.to_json(orient='records', indent=2)

def find_info() -> pd.DataFrame:
  query = f"""
    SELECT
  specialty_diseases,
  states,
  extract(year from `delta_hack_1`.mimilabs.cpsc_combined.contract_effective_date) as year,
  extract(month from `delta_hack_1`.mimilabs.cpsc_combined.contract_effective_date) as month,
  count(distinct `delta_hack_1`.mimilabs.snpdata.plan_id) as distinct_plans,
  count( `delta_hack_1`.mimilabs.snpdata.contract_number) as distinct_contracts,
  count( `delta_hack_1`.mimilabs.snpdata.plan_enrollment) as distinct_enrollments
FROM
  `delta_hack_1`.mimilabs.snpdata
  inner join
  `delta_hack_1`.mimilabs.mapd_plan_directory
  on
  `delta_hack_1`.mimilabs.snpdata.plan_type = `delta_hack_1`.mimilabs.mapd_plan_directory.plan_type
AND
  `delta_hack_1`.mimilabs.snpdata.contract_number = `delta_hack_1`.mimilabs.mapd_plan_directory.contract_number
  inner join
  `delta_hack_1`.mimilabs.cpsc_combined
  on
  `delta_hack_1`.mimilabs.snpdata.contract_number = `delta_hack_1`.mimilabs.cpsc_combined.contract_id
  and
  `delta_hack_1`.mimilabs.snpdata.plan_id = `delta_hack_1`.mimilabs.cpsc_combined.plan_id
GROUP BY
    specialty_diseases,
    states,
    year,
    month

  """
  return format_context(spark.sql(query).toPandas())
  

# === Agent A: ExtractionAgent ===
extraction_prompt = PromptTemplate.from_template(
  """
  You are a helpful healthcare assistant identifying what plans are available in my county. Categorise the speciality diseases into cardio, renal, mental, diabetic. One speciality disease can be mapped to multiple categories. Each speciality disease we map to at least one category. Return for each speciality disease the categories across states. Return just the list, not an explanation or summary.

  Here is the JSON data:
  {context}
  """
)

extraction_chain = (
    RunnableLambda(lambda _: {"context": find_info()})
    | extraction_prompt
    | llm
    | StrOutputParser()
    | RunnableLambda(lambda extracted: {"extracted_info": extracted})
)

def get_summary() -> pd.DataFrame:
  query = f"""
  select * from dataupload.default.final_pop_invest
  """
  return format_context(spark.sql(query).toPandas())

# === Agent B: SummaryAgent ===
summary_prompt = PromptTemplate.from_template(
    """
    Extracted Info has a column named state and you have already identified categories across same. The get_summary dataframe has state2 which is a statename. Now you have information about statewise disease categories, invesetments and population. Looking at all this together recommend which states we should invest more and which states we should invest less by co-relating the information. Return just the list, not an explanation or summary.  

    Extracted Info:
    {extracted_info}

    Population Data:
    {pop_data}
    """
)
summary_chain = (
    RunnableLambda(lambda inputs: {"extracted_info": inputs["extracted_info"], "pop_data": get_summary()})
    | summary_prompt
    | llm
    | StrOutputParser()
)

# === Agentic Chain ===
agentic_chain = (extraction_chain | summary_chain)

# Run the agentic chain
result = agentic_chain.invoke({})
print(result)
