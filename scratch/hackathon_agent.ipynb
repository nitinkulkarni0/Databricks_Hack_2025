{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U -qqq langchain_core langchain_databricks langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bca260b-13d1-448f-8082-30b60a85c9ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_databricks import ChatDatabricks\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from langchain_core.runnables import Runnable, RunnableSequence, RunnableLambda\n",
    "\n",
    "\n",
    "# configure workspace tokens\n",
    "w = WorkspaceClient()\n",
    "os.environ[\"DATABRICKS_HOST\"] = w.config.host\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = w.tokens.create(comment=\"for model serving\", lifetime_seconds=1200).token_value\n",
    "\n",
    "llm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-405b-instruct\")\n",
    "\n",
    "def format_context(df: pd.DataFrame) -> str:\n",
    "    return df.to_json(orient='records', indent=2)\n",
    "\n",
    "def find_info(location: str) -> pd.DataFrame:\n",
    "  query = f\"\"\"\n",
    "    SELECT\n",
    "  specialty_diseases,\n",
    "  states,\n",
    "  legal_entity_city,\n",
    "  extract(year from `dais-hackathon-2025`.mimilabs.cpsc_combined.contract_effective_date) as year,\n",
    "  extract(month from `dais-hackathon-2025`.mimilabs.cpsc_combined.contract_effective_date) as month,\n",
    "  count(distinct `dais-hackathon-2025`.mimilabs.snpdata.plan_id) as distinct_plans,\n",
    "  count( `dais-hackathon-2025`.mimilabs.snpdata.contract_number) as distinct_contracts,\n",
    "  count( `dais-hackathon-2025`.mimilabs.snpdata.plan_enrollment) as distinct_enrollments\n",
    "FROM\n",
    "  `dais-hackathon-2025`.mimilabs.snpdata\n",
    "  inner join\n",
    "  `dais-hackathon-2025`.mimilabs.mapd_plan_directory\n",
    "  on\n",
    "  `dais-hackathon-2025`.mimilabs.snpdata.plan_type = `dais-hackathon-2025`.mimilabs.mapd_plan_directory.plan_type\n",
    "AND\n",
    "  `dais-hackathon-2025`.mimilabs.snpdata.contract_number = `dais-hackathon-2025`.mimilabs.mapd_plan_directory.contract_number\n",
    "  inner join\n",
    "  `dais-hackathon-2025`.mimilabs.cpsc_combined\n",
    "  on\n",
    "  `dais-hackathon-2025`.mimilabs.snpdata.contract_number = `dais-hackathon-2025`.mimilabs.cpsc_combined.contract_id\n",
    "  and\n",
    "  `dais-hackathon-2025`.mimilabs.snpdata.plan_id = `dais-hackathon-2025`.mimilabs.cpsc_combined.plan_id\n",
    "GROUP BY\n",
    "    specialty_diseases,\n",
    "    states,\n",
    "    legal_entity_city,\n",
    "    year,\n",
    "    month\n",
    "\n",
    "  \"\"\"\n",
    "  return format_context(spark.sql(query).toPandas())\n",
    "  \n",
    "\n",
    "# === Agent A: ExtractionAgent ===\n",
    "extraction_prompt = PromptTemplate.from_template(\n",
    "  \"\"\"\n",
    "  You are a helpful healthcare assistant identifying what plans are available in my county. Categorise the speciality diseases into cardio, renal, mental, diabetic. One speciality disease can be mapped to multiple categories. Each speciality disease we map to at least one category. Return for each speciality disease the categories. Return just the list, not an explanation or summary.\n",
    "\n",
    "  Here is the JSON data:\n",
    "  {context}\n",
    "  \"\"\"\n",
    ")\n",
    "\n",
    "extraction_chain = (\n",
    "    find_info |\n",
    "    extraction_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "def get_summary(location: str) -> pd.DataFrame:\n",
    "  query = f\"\"\"\n",
    "  \n",
    "  \"\"\"\n",
    "  return format_context(spark.sql(query).toPandas())\n",
    "\n",
    "# === Agent B: SummaryAgent ===\n",
    "summary_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Based on the extracted categories return how many cardio categories we have.\n",
    "\n",
    "    Extracted Info:\n",
    "    {extracted_info}\n",
    "    \"\"\"\n",
    ")\n",
    "summary_chain = (\n",
    "    summary_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# === Agentic Chain ===\n",
    "agentic_chain = (\n",
    "    extraction_chain \n",
    "    | summary_chain | (lambda extracted: {\"extracted_info\": extracted}) \n",
    "    | summary_chain\n",
    ")\n",
    "\n",
    "# Run the agentic chain\n",
    "result = agentic_chain.invoke(\"Autauga\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "ipynb-notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
